# -*- coding: utf-8 -*-
"""
æµ‹è¯•MCP Google Newsæœç´¢åŠŸèƒ½
ä¸“é—¨ç”¨äºæ¼”ç¤ºé€šè¿‡MCPå·¥å…·è°ƒç”¨çœŸå®çš„è°·æ­Œæ–°é—»æœç´¢
"""

import os
import sys
import asyncio
from datetime import datetime, timedelta

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from tools.rag_tools import RAGTool
from tools.utility_tools import data_processor, file_utils
from tools.mcp.google_news_search import GoogleNewsSearchTool


async def test_google_news_search():
    """æµ‹è¯•Google Newsæœç´¢åŠŸèƒ½"""
    print("ğŸ” æµ‹è¯•MCP Google Newsæœç´¢åŠŸèƒ½")
    print("=" * 60)
    
    # ç§»é™¤WebSearchToolç›¸å…³å¯¼å…¥å’Œå®ä¾‹åŒ–ï¼Œæ”¹ä¸ºgoogle_news_search
    
    # æµ‹è¯•æœç´¢å…³é”®è¯
    test_keywords = [
        "å•†æ±¤ç§‘æŠ€ æœ€æ–°æ¶ˆæ¯",
        "SenseTime AIæŠ€æœ¯",
        "å•†æ±¤ç§‘æŠ€ è´¢æŠ¥"
    ]
    
    all_news = []
    
    for keyword in test_keywords:
        print(f"\nğŸ“° æœç´¢å…³é”®è¯: {keyword}")
        print("-" * 40)
        
        try:
            google_news_tool = GoogleNewsSearchTool()
            search_result = google_news_tool.execute(
                query=keyword,
                max_results=3,
                # start_date=(datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d"),
                # end_date=datetime.now().strftime("%Y-%m-%d")
            )
            
            print(f"âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(search_result.get('results', []))} æ¡æ–°é—»")
            
            # å¤„ç†æœç´¢ç»“æœ
            for i, result in enumerate(search_result.get('results', []), 1):
                print(f"\n{i}. {result.get('title', 'æ— æ ‡é¢˜')}")
                print(f"   æ¥æº: {result.get('source', 'æœªçŸ¥')}")
                print(f"   æ—¥æœŸ: {result.get('date', 'æœªçŸ¥')}")
                print(f"   æ‘˜è¦: {result.get('snippet', 'æ— æ‘˜è¦')[:100]}...")
                print(f"   URL: {result.get('url', 'æ— é“¾æ¥')}")
                
                # æ¸…ç†å’Œå­˜å‚¨æ–°é—»æ•°æ®
                cleaned_news = clean_news_data(result, keyword)
                if cleaned_news:
                    all_news.append(cleaned_news)
            
            # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            await asyncio.sleep(3)
            
        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")
            continue
    
    return all_news


def clean_news_data(raw_news: dict, keyword: str) -> dict:
    """æ¸…ç†æ–°é—»æ•°æ®"""
    try:
        title = raw_news.get('title', '')
        snippet = raw_news.get('snippet', '')
        url = raw_news.get('url', '')
        source = raw_news.get('source', '')
        date = raw_news.get('date', '')
        
        if not title:
            return None
        
        # æ¸…ç†æ–‡æœ¬
        cleaned_title = data_processor.clean_text(title)
        cleaned_snippet = data_processor.clean_text(snippet)
        
        return {
            'title': cleaned_title,
            'content': cleaned_snippet,
            'summary': cleaned_snippet[:200] + "..." if len(cleaned_snippet) > 200 else cleaned_snippet,
            'url': url,
            'source': source,
            'date': date,
            'keyword': keyword,
            'collected_at': datetime.now().isoformat()
        }
    except Exception as e:
        print(f"æ¸…ç†æ–°é—»æ•°æ®å¤±è´¥: {e}")
        return None


async def test_rag_integration(news_data: list):
    """æµ‹è¯•RAGé›†æˆ"""
    print(f"\nğŸ“š æµ‹è¯•RAGçŸ¥è¯†åº“é›†æˆ")
    print("=" * 60)
    
    # åˆ›å»ºRAGå·¥å…·å®ä¾‹
    rag_tool = RAGTool()
    
    # å°†æ–°é—»æ•°æ®å­˜å‚¨åˆ°RAGçŸ¥è¯†åº“
    for news in news_data:
        try:
            # åˆ›å»ºæ–‡æ¡£å†…å®¹
            doc_content = f"""
æ ‡é¢˜: {news['title']}
æ¥æº: {news['source']}
å…³é”®è¯: {news['keyword']}
æ—¥æœŸ: {news['date']}
æ‘˜è¦: {news['summary']}
å†…å®¹: {news['content']}
URL: {news['url']}
æ”¶é›†æ—¶é—´: {news['collected_at']}
"""
            
            # åˆ›å»ºå…ƒæ•°æ®
            doc_meta = {
                'type': 'news',
                'company': 'å•†æ±¤ç§‘æŠ€',
                'source': news['source'],
                'keyword': news['keyword'],
                'date': news['date'],
                'collected_at': news['collected_at']
            }
            
            # æ·»åŠ åˆ°RAGçŸ¥è¯†åº“
            result = rag_tool.add_document(doc_content, doc_meta)
            if result.get('success'):
                print(f"âœ… å·²å­˜å‚¨: {news['title'][:50]}...")
            else:
                print(f"âŒ å­˜å‚¨å¤±è´¥: {news['title'][:50]}...")
                
        except Exception as e:
            print(f"âŒ å­˜å‚¨æ–°é—»å¤±è´¥: {e}")
    
    # æµ‹è¯•RAGæœç´¢
    print(f"\nğŸ” æµ‹è¯•RAGçŸ¥è¯†åº“æœç´¢")
    print("-" * 40)
    
    search_queries = ["å•†æ±¤ç§‘æŠ€", "AIæŠ€æœ¯", "è´¢æŠ¥"]
    
    for query in search_queries:
        try:
            search_result = rag_tool.search_knowledge(query, top_k=3)
            print(f"\næœç´¢: '{query}'")
            print(f"æ‰¾åˆ° {search_result.get('total_results', 0)} æ¡ç›¸å…³ç»“æœ")
            
            for i, result in enumerate(search_result.get('results', [])[:2], 1):
                print(f"  {i}. {result.get('content', '')[:100]}...")
                
        except Exception as e:
            print(f"âŒ RAGæœç´¢å¤±è´¥: {e}")


async def generate_simple_report(news_data: list):
    """ç”Ÿæˆç®€å•çš„åˆ†ææŠ¥å‘Š"""
    print(f"\nğŸ“‹ ç”Ÿæˆç®€å•åˆ†ææŠ¥å‘Š")
    print("=" * 60)
    
    if not news_data:
        print("âŒ æ²¡æœ‰æ–°é—»æ•°æ®ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š")
        return
    
    # åˆ†ææ•°æ®
    sources = {}
    keywords = {}
    
    for news in news_data:
        source = news.get('source', 'æœªçŸ¥æ¥æº')
        keyword = news.get('keyword', 'æœªçŸ¥å…³é”®è¯')
        
        sources[source] = sources.get(source, 0) + 1
        keywords[keyword] = keywords.get(keyword, 0) + 1
    
    # ç”ŸæˆæŠ¥å‘Šå†…å®¹
    report_content = f"""
# å•†æ±¤ç§‘æŠ€æ–°é—»åˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}
**æ•°æ®æ¥æº**: Google Newsæœç´¢
**åˆ†æå‘¨æœŸ**: æœ€è¿‘30å¤©

## æ•°æ®æ¦‚è§ˆ

- æ€»æ–°é—»æ•°é‡: {len(news_data)} æ¡
- æ–°é—»æ¥æºæ•°é‡: {len(sources)} ä¸ª
- æœç´¢å…³é”®è¯æ•°é‡: {len(keywords)} ä¸ª

## æ–°é—»æ¥æºåˆ†å¸ƒ

{chr(10).join([f"- {source}: {count} æ¡" for source, count in sorted(sources.items(), key=lambda x: x[1], reverse=True)])}

## å…³é”®è¯åˆ†å¸ƒ

{chr(10).join([f"- {keyword}: {count} æ¡" for keyword, count in sorted(keywords.items(), key=lambda x: x[1], reverse=True)])}

## æœ€æ–°æ–°é—»æ‘˜è¦

"""
    
    # æ·»åŠ æœ€æ–°æ–°é—»
    for i, news in enumerate(news_data[:5], 1):
        report_content += f"""
### {i}. {news['title']}
- **æ¥æº**: {news['source']}
- **æ—¥æœŸ**: {news['date']}
- **æ‘˜è¦**: {news['summary']}
- **é“¾æ¥**: {news['url']}

"""
    
    report_content += """
---

**å…è´£å£°æ˜**: æœ¬æŠ¥å‘ŠåŸºäºGoogle Newsæœç´¢ç»“æœç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒã€‚
"""
    
    # ä¿å­˜æŠ¥å‘Š
    try:
        report_dir = "reports"
        file_utils.ensure_directory(report_dir)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"shangtang_mcp_test_report_{timestamp}.md"
        file_path = os.path.join(report_dir, filename)
        
        success = file_utils.write_file_safe(file_path, report_content, encoding='utf-8')
        
        if success:
            print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {file_path}")
            return file_path
        else:
            print("âŒ æŠ¥å‘Šä¿å­˜å¤±è´¥")
            
    except Exception as e:
        print(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
        return None


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– MCP Google Newsæœç´¢æµ‹è¯•")
    print("=" * 60)
    
    try:
        # 1. æµ‹è¯•Google Newsæœç´¢
        news_data = await test_google_news_search()
        
        if not news_data:
            print("âŒ æ²¡æœ‰è·å–åˆ°æ–°é—»æ•°æ®ï¼Œæµ‹è¯•ç»“æŸ")
            return
        
        print(f"\nâœ… æˆåŠŸæ”¶é›† {len(news_data)} æ¡æ–°é—»")
        
        # 2. æµ‹è¯•RAGé›†æˆ
        await test_rag_integration(news_data)
        
        # 3. ç”Ÿæˆç®€å•æŠ¥å‘Š
        report_path = await generate_simple_report(news_data)
        
        # 4. è¾“å‡ºæ€»ç»“
        print(f"\n" + "=" * 60)
        print("ğŸ“Š æµ‹è¯•æ€»ç»“")
        print("=" * 60)
        print(f"âœ… Google Newsæœç´¢: æˆåŠŸ")
        print(f"âœ… RAGçŸ¥è¯†åº“é›†æˆ: æˆåŠŸ")
        print(f"âœ… æŠ¥å‘Šç”Ÿæˆ: {'æˆåŠŸ' if report_path else 'å¤±è´¥'}")
        print(f"ğŸ“„ æŠ¥å‘Šæ–‡ä»¶: {report_path or 'æ— '}")
        print(f"ğŸ“° æ”¶é›†æ–°é—»: {len(news_data)} æ¡")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    
    print(f"\n" + "=" * 60)
    print("æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    asyncio.run(main()) 