# -*- coding: utf-8 -*-
"""
LLMHelperå•ç‹¬æµ‹è¯•ç¤ºä¾‹
ä¸ä¾èµ–CodeExecutorï¼Œåªæµ‹è¯•LLMHelperåŠŸèƒ½
"""

import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from agents.utils.llm_helper import LLMHelper
from config.llm_config import LLMConfig
from config.settings import get_settings


def test_llm_helper_import():
    """æµ‹è¯•LLMHelperå¯¼å…¥"""
    print("=== æµ‹è¯•LLMHelperå¯¼å…¥ ===")
    try:
        from agents.utils.llm_helper import LLMHelper
        from config.llm_config import LLMConfig
        print("âœ… LLMHelperå¯¼å…¥æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âŒ LLMHelperå¯¼å…¥å¤±è´¥: {e}")
        return False


def test_llm_config():
    """æµ‹è¯•LLMé…ç½®"""
    print("\n=== æµ‹è¯•LLMé…ç½® ===")
    try:
        # ä»è®¾ç½®åˆ›å»ºé…ç½®
        config = LLMConfig.from_settings()
        print(f"âœ… é…ç½®åˆ›å»ºæˆåŠŸ")
        print(f"   - æ¨¡å‹: {config.model}")
        print(f"   - æœ€å¤§token: {config.max_tokens}")
        print(f"   - æ¸©åº¦: {config.temperature}")
        print(f"   - APIå¯†é’¥: {'å·²è®¾ç½®' if config.api_key else 'æœªè®¾ç½®'}")
        return config
    except Exception as e:
        print(f"âŒ é…ç½®åˆ›å»ºå¤±è´¥: {e}")
        return None


def test_llm_helper_creation(config):
    """æµ‹è¯•LLMHelperåˆ›å»º"""
    print("\n=== æµ‹è¯•LLMHelperåˆ›å»º ===")
    try:
        if not config.api_key:
            print("âš ï¸ æœªè®¾ç½®APIå¯†é’¥ï¼Œè·³è¿‡å®é™…è°ƒç”¨æµ‹è¯•")
            return None
        
        llm = LLMHelper(config)
        print("âœ… LLMHelperåˆ›å»ºæˆåŠŸ")
        return llm
    except Exception as e:
        print(f"âŒ LLMHelperåˆ›å»ºå¤±è´¥: {e}")
        return None


def test_llm_call(llm):
    """æµ‹è¯•LLMè°ƒç”¨"""
    print("\n=== æµ‹è¯•LLMè°ƒç”¨ ===")
    if not llm:
        print("âš ï¸ LLMæœªåˆ›å»ºï¼Œè·³è¿‡è°ƒç”¨æµ‹è¯•")
        return
    
    try:
        # æµ‹è¯•åŒæ­¥è°ƒç”¨
        response = llm.call("è¯·ç”¨ä¸€å¥è¯ä»‹ç»äººå·¥æ™ºèƒ½")
        print(f"âœ… åŒæ­¥è°ƒç”¨æˆåŠŸ")
        print(f"   å›å¤: {response[:100]}...")
        
        # æµ‹è¯•YAMLè§£æ
        yaml_response = llm.call("è¯·ç”¨YAMLæ ¼å¼å›ç­”ï¼šåˆ—å‡ºä¸‰ä¸ªç¼–ç¨‹è¯­è¨€")
        parsed = llm.parse_yaml_response(yaml_response)
        print(f"âœ… YAMLè§£ææˆåŠŸ")
        print(f"   è§£æç»“æœ: {parsed}")
        
    except Exception as e:
        print(f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}")


def test_langchain_integration():
    """æµ‹è¯•LangChainé›†æˆ"""
    print("\n=== æµ‹è¯•LangChainé›†æˆ ===")
    try:
        from langchain_core.language_models.llms import LLM
        from agents.utils.llm_helper import LLMHelper
        
        # æ£€æŸ¥æ˜¯å¦ç»§æ‰¿è‡ªLangChain LLM
        llm = LLMHelper(LLMConfig(api_key="test"))
        if isinstance(llm, LLM):
            print("âœ… LLMHelperæ­£ç¡®ç»§æ‰¿è‡ªLangChain LLM")
        else:
            print("âŒ LLMHelperæœªæ­£ç¡®ç»§æ‰¿LangChain LLM")
            
        # æ£€æŸ¥æ˜¯å¦æœ‰å¿…éœ€çš„æ–¹æ³•
        required_methods = ['_call', '_acall', '_llm_type']
        for method in required_methods:
            if hasattr(llm, method):
                print(f"âœ… æ–¹æ³• {method} å­˜åœ¨")
            else:
                print(f"âŒ æ–¹æ³• {method} ç¼ºå¤±")
                
    except Exception as e:
        print(f"âŒ LangChainé›†æˆæµ‹è¯•å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– LLMHelperå•ç‹¬æµ‹è¯•\n")
    
    # æµ‹è¯•å¯¼å…¥
    if not test_llm_helper_import():
        return
    
    # æµ‹è¯•é…ç½®
    config = test_llm_config()
    if not config:
        return
    
    # æµ‹è¯•LLMHelperåˆ›å»º
    llm = test_llm_helper_creation(config)
    
    # æµ‹è¯•LLMè°ƒç”¨
    test_llm_call(llm)
    
    # æµ‹è¯•LangChainé›†æˆ
    test_langchain_integration()
    
    print("\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    main() 