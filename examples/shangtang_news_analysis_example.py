# -*- coding: utf-8 -*-
"""
å•†æ±¤ç§‘æŠ€æ–°é—»åˆ†æç¤ºä¾‹
å±•ç¤ºä¸¤ä¸ªæ™ºèƒ½ä½“é€šè¿‡A2A MCPå’ŒRAGæŠ€æœ¯å®ç°ä»è°·æ­Œæ–°é—»æœç´¢å•†æ±¤ç§‘æŠ€æ–°é—»æ•°æ®å¹¶ç”Ÿæˆè´¢æŠ¥

æ™ºèƒ½ä½“åˆ†å·¥ï¼š
1. æ–°é—»æ”¶é›†æ™ºèƒ½ä½“ (NewsCollectorAgent) - è´Ÿè´£æœç´¢å’Œæ”¶é›†æ–°é—»æ•°æ®
2. è´¢æŠ¥ç”Ÿæˆæ™ºèƒ½ä½“ (ReportGeneratorAgent) - è´Ÿè´£åˆ†ææ•°æ®å¹¶ç”Ÿæˆè´¢æŠ¥
"""

import os
import sys
import json
import asyncio
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from agents.base_agent import BaseAgent
from tools.mcp import WebSearchTool, FileOperationTool
from tools.rag_tools import RAGTool, Document
from tools.utility_tools import data_processor, file_utils, time_utils
from tools.mcp_tools import mcp_tool_manager


class NewsCollectorAgent(BaseAgent):
    """
    æ–°é—»æ”¶é›†æ™ºèƒ½ä½“
    èŒè´£: æœç´¢å•†æ±¤ç§‘æŠ€ç›¸å…³æ–°é—»ï¼Œæ”¶é›†å’Œé¢„å¤„ç†æ•°æ®
    å·¥å…·: ç½‘ç»œæœç´¢ã€æ•°æ®æ¸…ç†ã€RAGå­˜å‚¨
    """
    
    def __init__(self, name: str = "æ–°é—»æ”¶é›†æ™ºèƒ½ä½“", **kwargs):
        # é…ç½®æ–°é—»æ”¶é›†æ™ºèƒ½ä½“çš„ç‰¹å®šè®¾ç½®
        collector_config = {
            'llm': kwargs.get('llm', {
                'model': 'gpt-3.5-turbo', 
                'temperature': 0.3,
                'max_tokens': 2000
            }),
            'memory_type': 'buffer',
            'verbose': kwargs.get('verbose', True),
            'system_prompt': """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–°é—»æ”¶é›†æ™ºèƒ½ä½“ã€‚

ä½ çš„èŒè´£æ˜¯:
- ä½¿ç”¨ç½‘ç»œæœç´¢å·¥å…·æœç´¢å•†æ±¤ç§‘æŠ€ç›¸å…³çš„æœ€æ–°æ–°é—»
- æ”¶é›†å’Œæ•´ç†æ–°é—»æ•°æ®ï¼ŒåŒ…æ‹¬æ ‡é¢˜ã€å†…å®¹ã€æ¥æºã€æ—¶é—´ç­‰
- å¯¹æ–°é—»æ•°æ®è¿›è¡Œé¢„å¤„ç†å’Œæ¸…ç†
- å°†å¤„ç†åçš„æ•°æ®å­˜å‚¨åˆ°RAGçŸ¥è¯†åº“ä¸­
- ä¸è´¢æŠ¥ç”Ÿæˆæ™ºèƒ½ä½“åä½œï¼Œæä¾›é«˜è´¨é‡çš„æ•°æ®æ”¯æŒ

æœç´¢ç­–ç•¥:
- æœç´¢å…³é”®è¯ï¼šå•†æ±¤ç§‘æŠ€ã€SenseTimeã€AIæŠ€æœ¯ã€äººå·¥æ™ºèƒ½ã€è®¡ç®—æœºè§†è§‰
- é‡ç‚¹å…³æ³¨ï¼šå…¬å¸åŠ¨æ€ã€æŠ€æœ¯çªç ´ã€å¸‚åœºè¡¨ç°ã€è´¢åŠ¡æ•°æ®ã€åˆä½œä¼™ä¼´
- æ—¶é—´èŒƒå›´ï¼šæœ€è¿‘3ä¸ªæœˆå†…çš„æ–°é—»
- æ•°æ®è´¨é‡ï¼šä¼˜å…ˆé€‰æ‹©æƒå¨åª’ä½“å’Œå®˜æ–¹å‘å¸ƒçš„ä¿¡æ¯

è¯·ç¡®ä¿æ”¶é›†çš„æ•°æ®å‡†ç¡®ã€å…¨é¢ã€åŠæ—¶ã€‚""",
            'tools': self._get_collector_tools()
        }
        
        super().__init__(name, **collector_config)
        
        # åˆå§‹åŒ–å·¥å…·
        self.web_search_tool = WebSearchTool()
        self.file_tool = FileOperationTool()
        self.rag_tool = RAGTool()
        
        # æœç´¢é…ç½®
        self.search_keywords = [
            "å•†æ±¤ç§‘æŠ€ æœ€æ–°æ¶ˆæ¯",
            "SenseTime æ–°é—»",
            "å•†æ±¤ç§‘æŠ€ AIæŠ€æœ¯",
            "å•†æ±¤ç§‘æŠ€ è´¢æŠ¥",
            "å•†æ±¤ç§‘æŠ€ å¸‚åœºè¡¨ç°",
            "å•†æ±¤ç§‘æŠ€ åˆä½œä¼™ä¼´",
            "å•†æ±¤ç§‘æŠ€ æŠ€æœ¯çªç ´"
        ]
        
        self.logger.info(f"æ–°é—»æ”¶é›†æ™ºèƒ½ä½“ '{name}' åˆå§‹åŒ–å®Œæˆ")
    
    def _get_collector_tools(self) -> List[Dict[str, Any]]:
        """è·å–æ”¶é›†å·¥å…·é…ç½®"""
        return [
            {
                'type': 'function',
                'name': 'web_search',
                'description': 'ç½‘ç»œæœç´¢å·¥å…·'
            },
            {
                'type': 'function', 
                'name': 'file_operation',
                'description': 'æ–‡ä»¶æ“ä½œå·¥å…·'
            },
            {
                'type': 'function',
                'name': 'rag_storage',
                'description': 'RAGå­˜å‚¨å·¥å…·'
            }
        ]
    
    def _get_agent_description(self) -> str:
        return "ä¸“ä¸šçš„æ–°é—»æ”¶é›†æ™ºèƒ½ä½“ï¼Œè´Ÿè´£æœç´¢å’Œæ•´ç†å•†æ±¤ç§‘æŠ€ç›¸å…³æ–°é—»æ•°æ®"
    
    async def collect_news(self, max_results_per_keyword: int = 5) -> Dict[str, Any]:
        """æ”¶é›†æ–°é—»æ•°æ®"""
        self.logger.info("å¼€å§‹æ”¶é›†å•†æ±¤ç§‘æŠ€ç›¸å…³æ–°é—»...")
        
        all_news = []
        collected_count = 0
        
        for keyword in self.search_keywords:
            try:
                self.logger.info(f"æœç´¢å…³é”®è¯: {keyword}")
                
                # æ›¿æ¢åŸæœ‰çš„self.web_search_tool.executeè°ƒç”¨ä¸ºæ ‡å‡†MCPè°ƒç”¨
                search_result = mcp_tool_manager.execute_tool(
                    "web_search",
                    {
                        "query": keyword,
                        "max_results": max_results_per_keyword,
                        "search_engine": "google",
                        "search_type": "news",
                        "start_date": (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d"),
                        "end_date": datetime.now().strftime("%Y-%m-%d")
                    }
                )
                
                if search_result.get('results'):
                    for result in search_result['results']:
                        # æ¸…ç†å’Œé¢„å¤„ç†æ–°é—»æ•°æ®
                        cleaned_news = self._clean_news_data(result, keyword)
                        if cleaned_news:
                            all_news.append(cleaned_news)
                            collected_count += 1
                
                # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                await asyncio.sleep(2)
                
            except Exception as e:
                self.logger.error(f"æœç´¢å…³é”®è¯ '{keyword}' å¤±è´¥: {e}")
                continue
        
        # å­˜å‚¨åˆ°RAGçŸ¥è¯†åº“
        if all_news:
            self._store_news_to_rag(all_news)
        
        result = {
            'success': True,
            'total_collected': collected_count,
            'news_data': all_news,
            'search_keywords': self.search_keywords,
            'collection_time': datetime.now().isoformat()
        }
        
        self.logger.info(f"æ–°é—»æ”¶é›†å®Œæˆï¼Œå…±æ”¶é›† {collected_count} æ¡æ–°é—»")
        return result
    
    def _clean_news_data(self, raw_news: Dict[str, Any], keyword: str) -> Optional[Dict[str, Any]]:
        """æ¸…ç†å’Œé¢„å¤„ç†æ–°é—»æ•°æ®"""
        try:
            # æå–åŸºæœ¬ä¿¡æ¯ï¼ˆé€‚é…Google Newsæ ¼å¼ï¼‰
            title = raw_news.get('title', '')
            snippet = raw_news.get('snippet', '')  # Google Newsè¿”å›çš„æ˜¯snippetè€Œä¸æ˜¯content
            url = raw_news.get('url', '')
            source = raw_news.get('source', '')
            date = raw_news.get('date', '')
            
            # æ•°æ®éªŒè¯
            if not title:
                return None
            
            # æ¸…ç†æ–‡æœ¬å†…å®¹
            cleaned_title = data_processor.clean_text(title)
            cleaned_snippet = data_processor.clean_text(snippet)
            
            # ç”Ÿæˆæ‘˜è¦ï¼ˆå¦‚æœæ²¡æœ‰snippetï¼Œä½¿ç”¨æ ‡é¢˜ï¼‰
            summary = cleaned_snippet if cleaned_snippet else cleaned_title
            
            # æå–å…³é”®ä¿¡æ¯
            extracted_info = {
                'emails': data_processor.extract_emails(cleaned_snippet),
                'urls': data_processor.extract_urls(cleaned_snippet),
                'word_count': data_processor.count_words(cleaned_snippet),
                'language': data_processor.detect_language(cleaned_snippet)
            }
            
            cleaned_news = {
                'title': cleaned_title,
                'content': cleaned_snippet,  # ä½¿ç”¨snippetä½œä¸ºå†…å®¹
                'summary': summary,
                'url': url,
                'source': source,
                'date': date,
                'keyword': keyword,
                'extracted_info': extracted_info,
                'collected_at': datetime.now().isoformat(),
                'news_id': f"news_{time_utils.get_current_timestamp()}_{len(cleaned_title)}"
            }
            
            return cleaned_news
            
        except Exception as e:
            self.logger.error(f"æ¸…ç†æ–°é—»æ•°æ®å¤±è´¥: {e}")
            return None
    
    def _store_news_to_rag(self, news_list: List[Dict[str, Any]]):
        """å°†æ–°é—»æ•°æ®å­˜å‚¨åˆ°RAGçŸ¥è¯†åº“"""
        try:
            # æ”¶é›†åˆ°news_liståï¼Œæ‰¹é‡å…¥åº“
            contents = [news['content'] for news in news_list]
            doc_metas = [news for news in news_list]  # å‡è®¾newsæœ¬èº«å°±æ˜¯å…ƒæ•°æ®dict
            add_results = self.rag_tool.add_documents(contents, doc_metas)
            
            self.logger.info(f"æˆåŠŸå°† {len(news_list)} æ¡æ–°é—»æ‰¹é‡å­˜å‚¨åˆ°RAGçŸ¥è¯†åº“")
            
        except Exception as e:
            self.logger.error(f"æ‰¹é‡å­˜å‚¨æ–°é—»åˆ°RAGçŸ¥è¯†åº“å¤±è´¥: {e}")
    
    def get_news_summary(self) -> Dict[str, Any]:
        """è·å–æ–°é—»æ”¶é›†æ‘˜è¦"""
        try:
            # ä»RAGçŸ¥è¯†åº“æœç´¢å•†æ±¤ç§‘æŠ€ç›¸å…³æ–°é—»
            search_result = self.rag_tool.search_knowledge("å•†æ±¤ç§‘æŠ€", top_k=10)
            
            summary = {
                'total_news': len(search_result.get('results', [])),
                'sources': list(set([r.get('doc_meta', {}).get('source', '') for r in search_result.get('results', [])])),
                'keywords_covered': list(set([r.get('doc_meta', {}).get('keyword', '') for r in search_result.get('results', [])])),
                'latest_news': search_result.get('results', [])[:3],
                'collection_stats': {
                    'total_documents': len(search_result.get('results', [])),
                    'avg_word_count': sum([r.get('doc_meta', {}).get('word_count', 0) for r in search_result.get('results', [])]) / max(len(search_result.get('results', [])), 1)
                }
            }
            
            return summary
            
        except Exception as e:
            self.logger.error(f"è·å–æ–°é—»æ‘˜è¦å¤±è´¥: {e}")
            return {'error': str(e)}


class ReportGeneratorAgent(BaseAgent):
    """
    è´¢æŠ¥ç”Ÿæˆæ™ºèƒ½ä½“
    èŒè´£: åˆ†ææ–°é—»æ•°æ®ï¼Œç”Ÿæˆå•†æ±¤ç§‘æŠ€è´¢æŠ¥åˆ†æ
    å·¥å…·: RAGæ£€ç´¢ã€æ•°æ®åˆ†æã€æŠ¥å‘Šç”Ÿæˆ
    """
    
    def __init__(self, name: str = "è´¢æŠ¥ç”Ÿæˆæ™ºèƒ½ä½“", **kwargs):
        # é…ç½®è´¢æŠ¥ç”Ÿæˆæ™ºèƒ½ä½“çš„ç‰¹å®šè®¾ç½®
        generator_config = {
            'llm': kwargs.get('llm', {
                'model': 'gpt-4', 
                'temperature': 0.2,
                'max_tokens': 4000
            }),
            'memory_type': 'summary',
            'verbose': kwargs.get('verbose', True),
            'system_prompt': """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è´¢æŠ¥åˆ†ææ™ºèƒ½ä½“ã€‚

ä½ çš„èŒè´£æ˜¯:
- åˆ†æå•†æ±¤ç§‘æŠ€çš„æ–°é—»æ•°æ®ï¼Œè¯†åˆ«å…³é”®ä¸šåŠ¡åŠ¨æ€
- è¯„ä¼°å…¬å¸çš„è´¢åŠ¡çŠ¶å†µã€å¸‚åœºè¡¨ç°å’ŒæŠ€æœ¯å‘å±•
- ç”Ÿæˆä¸“ä¸šçš„è´¢æŠ¥åˆ†ææŠ¥å‘Š
- æä¾›æŠ•èµ„å»ºè®®å’Œé£é™©æç¤º
- ç¡®ä¿æŠ¥å‘Šçš„å‡†ç¡®æ€§å’Œä¸“ä¸šæ€§

åˆ†æç»´åº¦:
- ä¸šåŠ¡å‘å±•ï¼šæ–°äº§å“ã€æŠ€æœ¯çªç ´ã€å¸‚åœºæ‰©å¼ 
- è´¢åŠ¡çŠ¶å†µï¼šæ”¶å…¥ã€åˆ©æ¶¦ã€ç°é‡‘æµã€å€ºåŠ¡
- å¸‚åœºè¡¨ç°ï¼šè‚¡ä»·ã€å¸‚å€¼ã€æŠ•èµ„è€…ä¿¡å¿ƒ
- ç«äº‰ç¯å¢ƒï¼šè¡Œä¸šåœ°ä½ã€ç«äº‰å¯¹æ‰‹ã€å¸‚åœºæœºä¼š
- é£é™©å› ç´ ï¼šæ”¿ç­–é£é™©ã€æŠ€æœ¯é£é™©ã€å¸‚åœºé£é™©

æŠ¥å‘Šç»“æ„:
1. æ‰§è¡Œæ‘˜è¦
2. å…¬å¸æ¦‚å†µ
3. ä¸šåŠ¡åˆ†æ
4. è´¢åŠ¡åˆ†æ
5. å¸‚åœºåˆ†æ
6. é£é™©è¯„ä¼°
7. æŠ•èµ„å»ºè®®
8. é™„å½•

è¯·åŸºäºæ”¶é›†çš„æ–°é—»æ•°æ®ç”Ÿæˆä¸“ä¸šã€å®¢è§‚ã€å…¨é¢çš„è´¢æŠ¥åˆ†æã€‚""",
            'tools': self._get_generator_tools()
        }
        
        super().__init__(name, **generator_config)
        
        # åˆå§‹åŒ–å·¥å…·
        self.rag_tool = RAGTool()
        self.file_tool = FileOperationTool()
        
        self.logger.info(f"è´¢æŠ¥ç”Ÿæˆæ™ºèƒ½ä½“ '{name}' åˆå§‹åŒ–å®Œæˆ")
    
    def _get_generator_tools(self) -> List[Dict[str, Any]]:
        """è·å–ç”Ÿæˆå·¥å…·é…ç½®"""
        return [
            {
                'type': 'function',
                'name': 'rag_search',
                'description': 'RAGçŸ¥è¯†åº“æœç´¢å·¥å…·'
            },
            {
                'type': 'function',
                'name': 'file_operation',
                'description': 'æ–‡ä»¶æ“ä½œå·¥å…·'
            },
            {
                'type': 'function',
                'name': 'data_analysis',
                'description': 'æ•°æ®åˆ†æå·¥å…·'
            }
        ]
    
    def _get_agent_description(self) -> str:
        return "ä¸“ä¸šçš„è´¢æŠ¥åˆ†ææ™ºèƒ½ä½“ï¼Œè´Ÿè´£åˆ†ææ–°é—»æ•°æ®å¹¶ç”Ÿæˆå•†æ±¤ç§‘æŠ€è´¢æŠ¥æŠ¥å‘Š"
    
    async def generate_report(self, report_type: str = "comprehensive") -> Dict[str, Any]:
        """ç”Ÿæˆè´¢æŠ¥åˆ†ææŠ¥å‘Š"""
        self.logger.info(f"å¼€å§‹ç”Ÿæˆ{report_type}è´¢æŠ¥åˆ†ææŠ¥å‘Š...")
        
        try:
            # 1. ä»RAGçŸ¥è¯†åº“æ£€ç´¢ç›¸å…³æ–°é—»æ•°æ®
            news_data = await self._retrieve_news_data()
            
            if not news_data:
                return {
                    'success': False,
                    'error': 'æœªæ‰¾åˆ°è¶³å¤Ÿçš„æ–°é—»æ•°æ®è¿›è¡Œåˆ†æ'
                }
            
            # 2. åˆ†ææ–°é—»æ•°æ®
            analysis_result = await self._analyze_news_data(news_data)
            
            # 3. ç”ŸæˆæŠ¥å‘Šå†…å®¹
            report_content = await self._generate_report_content(analysis_result, report_type)
            
            # 4. ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
            file_path = await self._save_report_to_file(report_content, report_type)
            
            result = {
                'success': True,
                'report_type': report_type,
                'file_path': file_path,
                'analysis_summary': analysis_result.get('summary', {}),
                'generated_at': datetime.now().isoformat(),
                'report_content': report_content
            }
            
            self.logger.info(f"è´¢æŠ¥åˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæˆ: {file_path}")
            return result
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆè´¢æŠ¥åˆ†ææŠ¥å‘Šå¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    async def _retrieve_news_data(self) -> List[Dict[str, Any]]:
        """ä»RAGçŸ¥è¯†åº“æ£€ç´¢æ–°é—»æ•°æ®"""
        try:
            # æœç´¢å•†æ±¤ç§‘æŠ€ç›¸å…³æ–°é—»
            search_result = self.rag_tool.search_knowledge("å•†æ±¤ç§‘æŠ€", top_k=20)
            
            if not search_result.get('results'):
                return []
            
            # æŒ‰æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
            news_list = search_result['results']
            news_list.sort(key=lambda x: x.get('doc_meta', {}).get('collected_at', ''), reverse=True)
            
            return news_list
            
        except Exception as e:
            self.logger.error(f"æ£€ç´¢æ–°é—»æ•°æ®å¤±è´¥: {e}")
            return []
    
    async def _analyze_news_data(self, news_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†ææ–°é—»æ•°æ®"""
        try:
            analysis = {
                'total_news': len(news_data),
                'time_range': self._get_time_range(news_data),
                'sources': self._analyze_sources(news_data),
                'topics': self._analyze_topics(news_data),
                'sentiment': self._analyze_sentiment(news_data),
                'key_events': self._extract_key_events(news_data),
                'summary': {}
            }
            
            # ç”Ÿæˆåˆ†ææ‘˜è¦
            analysis['summary'] = {
                'business_highlights': self._extract_business_highlights(news_data),
                'financial_indicators': self._extract_financial_indicators(news_data),
                'market_performance': self._extract_market_performance(news_data),
                'risk_factors': self._extract_risk_factors(news_data)
            }
            
            return analysis
            
        except Exception as e:
            self.logger.error(f"åˆ†ææ–°é—»æ•°æ®å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _get_time_range(self, news_data: List[Dict[str, Any]]) -> Dict[str, str]:
        """è·å–æ–°é—»æ—¶é—´èŒƒå›´"""
        if not news_data:
            return {'start': '', 'end': ''}
        
        dates = [news.get('doc_meta', {}).get('collected_at', '') for news in news_data]
        dates = [d for d in dates if d]
        
        if dates:
            return {
                'start': min(dates),
                'end': max(dates)
            }
        return {'start': '', 'end': ''}
    
    def _analyze_sources(self, news_data: List[Dict[str, Any]]) -> Dict[str, int]:
        """åˆ†ææ–°é—»æ¥æºåˆ†å¸ƒ"""
        sources = {}
        for news in news_data:
            source = news.get('doc_meta', {}).get('source', 'æœªçŸ¥æ¥æº')
            sources[source] = sources.get(source, 0) + 1
        return sources
    
    def _analyze_topics(self, news_data: List[Dict[str, Any]]) -> Dict[str, int]:
        """åˆ†ææ–°é—»ä¸»é¢˜åˆ†å¸ƒ"""
        topics = {}
        for news in news_data:
            keyword = news.get('doc_meta', {}).get('keyword', '')
            topics[keyword] = topics.get(keyword, 0) + 1
        return topics
    
    def _analyze_sentiment(self, news_data: List[Dict[str, Any]]) -> Dict[str, int]:
        """åˆ†ææ–°é—»æƒ…æ„Ÿå€¾å‘ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # è¿™é‡Œåº”è¯¥ä½¿ç”¨æ›´å¤æ‚çš„æƒ…æ„Ÿåˆ†ææ¨¡å‹
        positive_keywords = ['å¢é•¿', 'çªç ´', 'æˆåŠŸ', 'åˆä½œ', 'åˆ›æ–°', 'é¢†å…ˆ']
        negative_keywords = ['ä¸‹è·Œ', 'äºæŸ', 'é£é™©', 'æŒ‘æˆ˜', 'ç«äº‰', 'å›°éš¾']
        
        sentiment = {'positive': 0, 'negative': 0, 'neutral': 0}
        
        for news in news_data:
            content = news.get('content', '').lower()
            positive_count = sum(1 for keyword in positive_keywords if keyword in content)
            negative_count = sum(1 for keyword in negative_keywords if keyword in content)
            
            if positive_count > negative_count:
                sentiment['positive'] += 1
            elif negative_count > positive_count:
                sentiment['negative'] += 1
            else:
                sentiment['neutral'] += 1
        
        return sentiment
    
    def _extract_key_events(self, news_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æå–å…³é”®äº‹ä»¶"""
        key_events = []
        for news in news_data[:10]:  # å–å‰10æ¡ä½œä¸ºå…³é”®äº‹ä»¶
            event = {
                'title': news.get('title', ''),
                'summary': news.get('summary', ''),
                'source': news.get('doc_meta', {}).get('source', ''),
                'date': news.get('doc_meta', {}).get('collected_at', ''),
                'url': news.get('url', '')
            }
            key_events.append(event)
        return key_events
    
    def _extract_business_highlights(self, news_data: List[Dict[str, Any]]) -> List[str]:
        """æå–ä¸šåŠ¡äº®ç‚¹"""
        highlights = []
        for news in news_data:
            title = news.get('title', '')
            content = news.get('content', '')
            
            # ç®€å•çš„å…³é”®è¯åŒ¹é…
            if any(keyword in title or keyword in content for keyword in ['åˆä½œ', 'ç­¾çº¦', 'å‘å¸ƒ', 'æ¨å‡º', 'çªç ´']):
                highlights.append(news.get('summary', title))
        
        return highlights[:5]  # è¿”å›å‰5ä¸ªäº®ç‚¹
    
    def _extract_financial_indicators(self, news_data: List[Dict[str, Any]]) -> List[str]:
        """æå–è´¢åŠ¡æŒ‡æ ‡"""
        indicators = []
        for news in news_data:
            content = news.get('content', '')
            
            # ç®€å•çš„è´¢åŠ¡å…³é”®è¯åŒ¹é…
            if any(keyword in content for keyword in ['è¥æ”¶', 'åˆ©æ¶¦', 'å¢é•¿', 'æŠ•èµ„', 'èèµ„']):
                indicators.append(news.get('summary', news.get('title', '')))
        
        return indicators[:5]
    
    def _extract_market_performance(self, news_data: List[Dict[str, Any]]) -> List[str]:
        """æå–å¸‚åœºè¡¨ç°"""
        performance = []
        for news in news_data:
            content = news.get('content', '')
            
            # ç®€å•çš„å¸‚åœºå…³é”®è¯åŒ¹é…
            if any(keyword in content for keyword in ['è‚¡ä»·', 'å¸‚å€¼', 'å¸‚åœº', 'ä»½é¢', 'ç«äº‰']):
                performance.append(news.get('summary', news.get('title', '')))
        
        return performance[:5]
    
    def _extract_risk_factors(self, news_data: List[Dict[str, Any]]) -> List[str]:
        """æå–é£é™©å› ç´ """
        risks = []
        for news in news_data:
            content = news.get('content', '')
            
            # ç®€å•çš„é£é™©å…³é”®è¯åŒ¹é…
            if any(keyword in content for keyword in ['é£é™©', 'æŒ‘æˆ˜', 'å›°éš¾', 'é—®é¢˜', 'ç›‘ç®¡']):
                risks.append(news.get('summary', news.get('title', '')))
        
        return risks[:5]
    
    async def _generate_report_content(self, analysis_result: Dict[str, Any], report_type: str) -> str:
        """ç”ŸæˆæŠ¥å‘Šå†…å®¹"""
        try:
            # æ„å»ºæŠ¥å‘Šæ¨¡æ¿
            report_template = f"""
# å•†æ±¤ç§‘æŠ€è´¢æŠ¥åˆ†ææŠ¥å‘Š

**æŠ¥å‘Šç±»å‹**: {report_type}åˆ†ææŠ¥å‘Š  
**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}  
**æ•°æ®æ¥æº**: æ–°é—»æ•°æ®æŒ–æ˜ä¸åˆ†æ  
**åˆ†æå‘¨æœŸ**: {analysis_result.get('time_range', {}).get('start', '')} è‡³ {analysis_result.get('time_range', {}).get('end', '')}

## æ‰§è¡Œæ‘˜è¦

æœ¬æŠ¥å‘ŠåŸºäºå¯¹å•†æ±¤ç§‘æŠ€ç›¸å…³æ–°é—»æ•°æ®çš„æ·±åº¦åˆ†æï¼Œä»ä¸šåŠ¡å‘å±•ã€è´¢åŠ¡çŠ¶å†µã€å¸‚åœºè¡¨ç°ç­‰å¤šä¸ªç»´åº¦å¯¹å…¬å¸è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚

**å…³é”®å‘ç°**:
- å…±åˆ†æ {analysis_result.get('total_news', 0)} æ¡ç›¸å…³æ–°é—»
- è¦†ç›– {len(analysis_result.get('sources', {}))} ä¸ªæ–°é—»æ¥æº
- è¯†åˆ« {len(analysis_result.get('key_events', []))} ä¸ªå…³é”®äº‹ä»¶

## 1. å…¬å¸æ¦‚å†µ

å•†æ±¤ç§‘æŠ€ï¼ˆSenseTimeï¼‰æ˜¯å…¨çƒé¢†å…ˆçš„äººå·¥æ™ºèƒ½å…¬å¸ï¼Œä¸“æ³¨äºè®¡ç®—æœºè§†è§‰å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯ã€‚

## 2. ä¸šåŠ¡åˆ†æ

### 2.1 ä¸šåŠ¡äº®ç‚¹
{self._format_list(analysis_result.get('summary', {}).get('business_highlights', []))}

### 2.2 æŠ€æœ¯å‘å±•
åŸºäºæ–°é—»æ•°æ®åˆ†æï¼Œå•†æ±¤ç§‘æŠ€åœ¨ä»¥ä¸‹æŠ€æœ¯é¢†åŸŸè¡¨ç°çªå‡ºï¼š
- è®¡ç®—æœºè§†è§‰æŠ€æœ¯
- æ·±åº¦å­¦ä¹ ç®—æ³•
- AIèŠ¯ç‰‡ç ”å‘
- è¡Œä¸šè§£å†³æ–¹æ¡ˆ

## 3. è´¢åŠ¡åˆ†æ

### 3.1 è´¢åŠ¡æŒ‡æ ‡
{self._format_list(analysis_result.get('summary', {}).get('financial_indicators', []))}

### 3.2 è´¢åŠ¡è¡¨ç°è¯„ä¼°
åŸºäºæ–°é—»æ•°æ®åˆ†æï¼Œå…¬å¸è´¢åŠ¡çŠ¶å†µæ€»ä½“ç¨³å¥ï¼Œä½†éœ€è¦å…³æ³¨ä»¥ä¸‹æ–¹é¢ï¼š
- æ”¶å…¥å¢é•¿è¶‹åŠ¿
- ç›ˆåˆ©èƒ½åŠ›å˜åŒ–
- ç°é‡‘æµçŠ¶å†µ
- æŠ•èµ„æ´»åŠ¨

## 4. å¸‚åœºåˆ†æ

### 4.1 å¸‚åœºè¡¨ç°
{self._format_list(analysis_result.get('summary', {}).get('market_performance', []))}

### 4.2 ç«äº‰ç¯å¢ƒ
- è¡Œä¸šåœ°ä½ï¼šAIé¢†åŸŸé¢†å…ˆä¼ä¸š
- ä¸»è¦ç«äº‰å¯¹æ‰‹ï¼šç™¾åº¦ã€è…¾è®¯ã€é˜¿é‡Œå·´å·´ç­‰
- å¸‚åœºæœºä¼šï¼šæ•°å­—åŒ–è½¬å‹ã€AI+è¡Œä¸šåº”ç”¨

## 5. é£é™©è¯„ä¼°

### 5.1 ä¸»è¦é£é™©å› ç´ 
{self._format_list(analysis_result.get('summary', {}).get('risk_factors', []))}

### 5.2 é£é™©ç­‰çº§è¯„ä¼°
- æŠ€æœ¯é£é™©ï¼šä¸­ç­‰
- å¸‚åœºé£é™©ï¼šä¸­ç­‰
- æ”¿ç­–é£é™©ï¼šä¸­ç­‰
- ç«äº‰é£é™©ï¼šé«˜

## 6. æŠ•èµ„å»ºè®®

### 6.1 æŠ•èµ„è¯„çº§
åŸºäºæ–°é—»æ•°æ®åˆ†æï¼Œå»ºè®®æŠ•èµ„è¯„çº§ï¼š**è°¨æ…ä¹è§‚**

### 6.2 æŠ•èµ„ç†ç”±
- æŠ€æœ¯å®åŠ›å¼ºï¼Œåœ¨AIé¢†åŸŸå…·æœ‰é¢†å…ˆä¼˜åŠ¿
- ä¸šåŠ¡å¸ƒå±€å…¨é¢ï¼Œè¦†ç›–å¤šä¸ªè¡Œä¸šåº”ç”¨
- åˆä½œä¼™ä¼´ä¼—å¤šï¼Œç”Ÿæ€ä½“ç³»å®Œå–„

### 6.3 é£é™©æç¤º
- è¡Œä¸šç«äº‰æ¿€çƒˆï¼Œéœ€è¦æŒç»­æŠ€æœ¯åˆ›æ–°
- æ”¿ç­–ç¯å¢ƒå˜åŒ–å¯èƒ½å½±å“ä¸šåŠ¡å‘å±•
- ç›ˆåˆ©èƒ½åŠ›éœ€è¦è¿›ä¸€æ­¥æ”¹å–„

## 7. é™„å½•

### 7.1 å…³é”®äº‹ä»¶æ—¶é—´çº¿
{self._format_events(analysis_result.get('key_events', []))}

### 7.2 æ–°é—»æ¥æºåˆ†å¸ƒ
{self._format_sources(analysis_result.get('sources', {}))}

### 7.3 æƒ…æ„Ÿåˆ†æç»“æœ
{self._format_sentiment(analysis_result.get('sentiment', {}))}

---

**å…è´£å£°æ˜**: æœ¬æŠ¥å‘ŠåŸºäºå…¬å¼€æ–°é—»æ•°æ®ç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚æŠ•èµ„æœ‰é£é™©ï¼Œå†³ç­–éœ€è°¨æ…ã€‚
"""
            
            return report_template
            
        except Exception as e:
            self.logger.error(f"ç”ŸæˆæŠ¥å‘Šå†…å®¹å¤±è´¥: {e}")
            return f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def _format_list(self, items: List[str]) -> str:
        """æ ¼å¼åŒ–åˆ—è¡¨"""
        if not items:
            return "- æš‚æ— ç›¸å…³æ•°æ®"
        return "\n".join([f"- {item}" for item in items])
    
    def _format_events(self, events: List[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–äº‹ä»¶åˆ—è¡¨"""
        if not events:
            return "- æš‚æ— å…³é”®äº‹ä»¶"
        
        formatted = []
        for i, event in enumerate(events, 1):
            formatted.append(f"{i}. **{event.get('title', '')}**")
            formatted.append(f"   - æ—¶é—´: {event.get('date', '')}")
            formatted.append(f"   - æ¥æº: {event.get('source', '')}")
            formatted.append(f"   - æ‘˜è¦: {event.get('summary', '')}")
            formatted.append("")
        
        return "\n".join(formatted)
    
    def _format_sources(self, sources: Dict[str, int]) -> str:
        """æ ¼å¼åŒ–æ¥æºåˆ†å¸ƒ"""
        if not sources:
            return "- æš‚æ— æ¥æºæ•°æ®"
        
        formatted = []
        for source, count in sorted(sources.items(), key=lambda x: x[1], reverse=True):
            formatted.append(f"- {source}: {count} æ¡æ–°é—»")
        
        return "\n".join(formatted)
    
    def _format_sentiment(self, sentiment: Dict[str, int]) -> str:
        """æ ¼å¼åŒ–æƒ…æ„Ÿåˆ†æ"""
        total = sum(sentiment.values())
        if total == 0:
            return "- æš‚æ— æƒ…æ„Ÿåˆ†ææ•°æ®"
        
        formatted = []
        for key, value in sentiment.items():
            percentage = (value / total) * 100
            formatted.append(f"- {key}: {value} æ¡ ({percentage:.1f}%)")
        
        return "\n".join(formatted)
    
    async def _save_report_to_file(self, report_content: str, report_type: str) -> str:
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        try:
            # åˆ›å»ºæŠ¥å‘Šç›®å½•
            report_dir = "reports"
            file_utils.ensure_directory(report_dir)
            
            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"shangtang_report_{report_type}_{timestamp}.md"
            file_path = os.path.join(report_dir, filename)
            
            # ä¿å­˜æ–‡ä»¶
            success = file_utils.write_file_safe(file_path, report_content, encoding='utf-8')
            
            if success:
                self.logger.info(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {file_path}")
                return file_path
            else:
                raise Exception("æ–‡ä»¶ä¿å­˜å¤±è´¥")
                
        except Exception as e:
            self.logger.error(f"ä¿å­˜æŠ¥å‘Šæ–‡ä»¶å¤±è´¥: {e}")
            raise e


class A2ACommunication:
    """
    A2Aé€šä¿¡ç®¡ç†å™¨
    è´Ÿè´£ä¸¤ä¸ªæ™ºèƒ½ä½“ä¹‹é—´çš„é€šä¿¡å’Œåä½œ
    """
    
    def __init__(self):
        self.news_collector = NewsCollectorAgent()
        self.report_generator = ReportGeneratorAgent()
        self.communication_log = []
    
    async def execute_full_workflow(self) -> Dict[str, Any]:
        """æ‰§è¡Œå®Œæ•´çš„å·¥ä½œæµç¨‹"""
        print("ğŸš€ å¼€å§‹å•†æ±¤ç§‘æŠ€æ–°é—»åˆ†æå·¥ä½œæµç¨‹...")
        
        try:
            # 1. æ–°é—»æ”¶é›†é˜¶æ®µ
            print("\nğŸ“° ç¬¬ä¸€é˜¶æ®µï¼šæ–°é—»æ•°æ®æ”¶é›†")
            collection_result = await self.news_collector.collect_news(max_results_per_keyword=5)
            
            if not collection_result.get('success'):
                return {
                    'success': False,
                    'error': f"æ–°é—»æ”¶é›†å¤±è´¥: {collection_result.get('error', 'æœªçŸ¥é”™è¯¯')}"
                }
            
            print(f"âœ… æ–°é—»æ”¶é›†å®Œæˆï¼Œå…±æ”¶é›† {collection_result.get('total_collected', 0)} æ¡æ–°é—»")
            
            # 2. æ•°æ®ä¼ é€’å’ŒéªŒè¯
            print("\nğŸ“Š ç¬¬äºŒé˜¶æ®µï¼šæ•°æ®éªŒè¯å’Œå‡†å¤‡")
            news_summary = self.news_collector.get_news_summary()
            print(f"ğŸ“ˆ æ•°æ®æ‘˜è¦: {news_summary.get('total_news', 0)} æ¡æ–°é—»ï¼Œ{len(news_summary.get('sources', []))} ä¸ªæ¥æº")
            
            # 3. è´¢æŠ¥ç”Ÿæˆé˜¶æ®µ
            print("\nğŸ“‹ ç¬¬ä¸‰é˜¶æ®µï¼šè´¢æŠ¥åˆ†æç”Ÿæˆ")
            report_result = await self.report_generator.generate_report(report_type="comprehensive")
            
            if not report_result.get('success'):
                return {
                    'success': False,
                    'error': f"è´¢æŠ¥ç”Ÿæˆå¤±è´¥: {report_result.get('error', 'æœªçŸ¥é”™è¯¯')}"
                }
            
            print(f"âœ… è´¢æŠ¥ç”Ÿæˆå®Œæˆ: {report_result.get('file_path', '')}")
            
            # 4. ç”Ÿæˆæœ€ç»ˆç»“æœ
            final_result = {
                'success': True,
                'workflow_completed': True,
                'collection_result': collection_result,
                'report_result': report_result,
                'news_summary': news_summary,
                'completed_at': datetime.now().isoformat(),
                'total_duration': 'å·¥ä½œæµç¨‹æ‰§è¡Œå®Œæˆ'
            }
            
            print("\nğŸ‰ å·¥ä½œæµç¨‹æ‰§è¡Œå®Œæˆï¼")
            print(f"ğŸ“„ æŠ¥å‘Šæ–‡ä»¶: {report_result.get('file_path', '')}")
            
            return final_result
            
        except Exception as e:
            error_msg = f"å·¥ä½œæµç¨‹æ‰§è¡Œå¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            return {
                'success': False,
                'error': error_msg
            }


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– å•†æ±¤ç§‘æŠ€æ–°é—»åˆ†æç¤ºä¾‹")
    print("=" * 50)
    
    # åˆ›å»ºA2Aé€šä¿¡ç®¡ç†å™¨
    a2a_manager = A2ACommunication()
    
    # æ‰§è¡Œå®Œæ•´å·¥ä½œæµç¨‹
    result = await a2a_manager.execute_full_workflow()
    
    # è¾“å‡ºç»“æœæ‘˜è¦
    print("\n" + "=" * 50)
    print("ğŸ“Š æ‰§è¡Œç»“æœæ‘˜è¦")
    print("=" * 50)
    
    if result.get('success'):
        print("âœ… å·¥ä½œæµç¨‹æ‰§è¡ŒæˆåŠŸ")
        print(f"ğŸ“° æ”¶é›†æ–°é—»: {result.get('collection_result', {}).get('total_collected', 0)} æ¡")
        print(f"ğŸ“„ ç”ŸæˆæŠ¥å‘Š: {result.get('report_result', {}).get('file_path', '')}")
        print(f"â° å®Œæˆæ—¶é—´: {result.get('completed_at', '')}")
    else:
        print("âŒ å·¥ä½œæµç¨‹æ‰§è¡Œå¤±è´¥")
        print(f"é”™è¯¯ä¿¡æ¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
    
    print("\n" + "=" * 50)
    print("ç¤ºä¾‹æ‰§è¡Œå®Œæˆï¼")


if __name__ == "__main__":
    # è¿è¡Œç¤ºä¾‹
    asyncio.run(main()) 